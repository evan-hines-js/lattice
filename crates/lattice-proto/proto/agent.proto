// Lattice Agent-Cell Communication Protocol
//
// This defines the gRPC services between workload cluster agents and their
// parent cell (management cluster). All connections are initiated OUTBOUND
// from workload clusters - cells never connect to agents.
//
// Pivot flow (Cell → Agent):
// 1. Cell discovers CAPI CRDs, lists objects, builds ownership graph
// 2. Cell computes move sequence (topological sort)
// 3. Cell pauses Clusters/ClusterClasses on source
// 4. For each batch: Cell sends MoveObjectBatch → Agent creates objects, returns MoveObjectAck
// 5. Cell sends MoveComplete → Agent unpauses Clusters/ClusterClasses
// 6. Cell deletes source objects (reverse order, with finalizer removal)
//
// Unpivot flow (Agent → Cell) - same logic, reversed direction:
// 1. Agent discovers CAPI CRDs, lists objects, builds ownership graph
// 2. Agent pauses Clusters/ClusterClasses on source
// 3. Agent sends ClusterDeleting with structured MoveObjects
// 4. Cell creates objects with UID remapping (same as AgentMover)
// 5. Cell unpauses, deletes LatticeCluster
// 6. Agent source objects are garbage collected when cluster is deleted

syntax = "proto3";
package lattice.agent.v1;

import "google/protobuf/timestamp.proto";

// =============================================================================
// Services
// =============================================================================

// LatticeAgent service - agent connects to cell
//
// The agent initiates an outbound connection to the cell's gRPC server.
// This establishes a stream for control messages.
service LatticeAgent {
  // Bidirectional stream for control messages (heartbeat, commands, status)
  // Agent initiates, maintains persistent connection
  rpc StreamMessages(stream AgentMessage) returns (stream CellCommand);
}

// =============================================================================
// Control Messages: Agent to Cell
// =============================================================================

// Wrapper for all agent-to-cell messages
message AgentMessage {
  // Name of the cluster this agent manages
  string cluster_name = 1;

  oneof payload {
    AgentReady ready = 2;
    BootstrapComplete bootstrap_complete = 3;
    Heartbeat heartbeat = 4;
    ClusterHealth cluster_health = 5;
    StatusResponse status_response = 6;
    ClusterDeleting cluster_deleting = 7;
    KubernetesResponse kubernetes_response = 8;
    // Distributed move: ack after batch created
    MoveObjectAck move_ack = 9;
    // Distributed move: ack after unpause complete
    MoveCompleteAck move_complete_ack = 10;
    // Subtree state bubbling for routing
    SubtreeState subtree_state = 11;
    // Exec/attach/portforward: stdout/stderr data from agent
    ExecData exec_data = 12;
  }
}

// Sent when cluster deletion is initiated (unpivot)
//
// Unpivot is pivot in reverse: agent discovers CAPI resources, builds graph,
// sends structured objects to cell. Cell creates them with UID remapping,
// unpauses, then deletes. This ensures parent has all resources including
// any nodes added post-pivot.
message ClusterDeleting {
  // Namespace where CAPI resources live (on child) / will be created (on parent)
  string namespace = 1;
  // CAPI objects to move (same format as MoveObject for consistency)
  repeated MoveObject objects = 2;
  // Cluster name being deleted
  string cluster_name = 3;
}

// Sent when agent first connects to cell
message AgentReady {
  // Version of the lattice agent binary
  string agent_version = 1;
  // Kubernetes version running on this cluster
  string kubernetes_version = 2;
  // Current state of the agent
  AgentState state = 3;
  // API server endpoint (for informational purposes)
  string api_server_endpoint = 4;
}

// Sent when agent has completed initial setup
// (CAPI provider installed, controller reconciling)
message BootstrapComplete {
  // CAPI provider successfully installed based on LatticeCluster spec
  bool capi_ready = 1;
  // List of installed providers (e.g., ["docker", "kubeadm"])
  repeated string installed_providers = 2;
}

// Periodic heartbeat with agent state
message Heartbeat {
  // Current agent state
  AgentState state = 1;
  // Timestamp of this heartbeat
  google.protobuf.Timestamp timestamp = 2;
  // Uptime in seconds
  int64 uptime_seconds = 3;
}

// Cluster health information
message ClusterHealth {
  // Number of ready nodes
  int32 ready_nodes = 1;
  // Total number of nodes
  int32 total_nodes = 2;
  // Ready control plane nodes
  int32 ready_control_plane = 3;
  // Total control plane nodes
  int32 total_control_plane = 4;
  // Kubernetes conditions (Ready, MemoryPressure, etc.)
  repeated NodeCondition conditions = 5;
}

// Response to StatusRequest
message StatusResponse {
  // Correlates to the StatusRequest command_id
  string request_id = 1;
  // Current agent state
  AgentState state = 2;
  // Cluster health
  ClusterHealth health = 3;
  // CAPI resource counts (if requested)
  CapiStatus capi_status = 4;
}

// Kubernetes node condition
//
// Represents the status of a Kubernetes node condition such as Ready,
// MemoryPressure, DiskPressure, PIDPressure, or NetworkUnavailable.
message NodeCondition {
  // Condition type (e.g., "Ready", "MemoryPressure", "DiskPressure")
  string type = 1;
  // Condition status: "True", "False", or "Unknown"
  string status = 2;
  // Machine-readable reason for the condition's last transition
  string reason = 3;
  // Human-readable message with details about the last transition
  string message = 4;
}

// CAPI resource status
//
// Summary of Cluster API resources managed by this cluster. Used for
// health reporting and debugging pivot/unpivot operations.
message CapiStatus {
  // Number of Cluster resources (typically 1 for self-managed clusters)
  int32 cluster_count = 1;
  // Number of Machine resources (control plane + workers)
  int32 machine_count = 2;
  // Number of MachineDeployment resources (worker pools)
  int32 machine_deployment_count = 3;
  // Whether a KubeadmControlPlane resource exists
  bool has_control_plane = 4;
}

// Agent state machine
enum AgentState {
  AGENT_STATE_UNKNOWN = 0;
  // Cluster infrastructure is being provisioned by CAPI
  AGENT_STATE_PROVISIONING = 1;
  // CAPI resources are being imported (pivot in progress)
  AGENT_STATE_PIVOTING = 2;
  // Cluster is fully operational and self-managing
  AGENT_STATE_READY = 3;
  // Cluster has issues but is still operational
  AGENT_STATE_DEGRADED = 4;
  // Cluster is in a failed state
  AGENT_STATE_FAILED = 5;
}

// =============================================================================
// Control Messages: Cell to Agent
// =============================================================================

// Wrapper for all cell-to-agent commands
message CellCommand {
  // Unique ID for this command (for idempotency and correlation)
  string command_id = 1;

  oneof command {
    ApplyManifestsCommand apply_manifests = 2;
    StatusRequest status_request = 3;
    SyncDistributedResourcesCommand sync_resources = 4;
    KubernetesRequest kubernetes_request = 5;
    // Distributed move: batch of objects to create
    MoveObjectBatch move_batch = 6;
    // Distributed move: all batches sent, unpause target
    MoveComplete move_complete = 7;
    // Exec/attach/portforward: initiate exec session
    ExecRequest exec_request = 8;
    // Exec/attach/portforward: stdin data from client
    ExecData exec_stdin = 9;
    // Exec/attach/portforward: terminal resize
    ExecResize exec_resize = 10;
    // Exec/attach/portforward: cancel session (client disconnected)
    ExecCancel exec_cancel = 11;
  }
}

message ApplyManifestsCommand {
  repeated bytes manifests = 1;
}

// Reusable bundle of distributable resource types
//
// These resources are distributed from parent clusters to children during
// pivot operations and ongoing synchronization. They include infrastructure
// providers, secrets, and policy resources that children need to operate.
message DistributableResources {
  // CloudProvider CRDs (JSON) - infrastructure provider configurations
  repeated bytes cloud_providers = 1;
  // SecretsProvider CRDs (JSON) - external secrets integrations
  repeated bytes secrets_providers = 2;
  // Secrets referenced by providers (JSON) - credentials for providers
  repeated bytes secrets = 3;
  // CedarPolicy CRDs (JSON) - inherited authorization policies from ancestors
  repeated bytes cedar_policies = 4;
  // OIDCProvider CRDs (JSON) - inherited identity providers from ancestors
  repeated bytes oidc_providers = 5;
}

// Sync distributable resources to child cluster
// Used for both watch-triggered updates and periodic reconciliation
message SyncDistributedResourcesCommand {
  // Resources to sync to the child cluster
  DistributableResources resources = 1;
  // If true, delete resources not in this list (full sync)
  // If false, only add/update (incremental)
  bool full_sync = 2;
}

// Request current cluster status
message StatusRequest {
  // Whether to include detailed node information
  bool include_nodes = 1;
  // Whether to include CAPI resource status
  bool include_capi = 2;
}

// =============================================================================
// K8s API Proxy Messages
// =============================================================================

// Cell → Agent: Request to execute against child's K8s API
// Enables parent to access child cluster K8s API through the gRPC stream.
message KubernetesRequest {
  // Unique ID for correlation (UUID, never reused in session)
  string request_id = 1;

  // HTTP verb: GET, LIST, POST, PUT, PATCH, DELETE
  // LIST is GET with collection path, WATCH is GET with ?watch=true
  string verb = 2;

  // API path: /api/v1/namespaces/default/pods/nginx
  // /apis/apps/v1/namespaces/default/deployments
  string path = 3;

  // Query string (URL-encoded): watch=true&labelSelector=app%3Dnginx
  string query = 4;

  // Request body for POST/PUT/PATCH (JSON or YAML)
  bytes body = 5;

  // Content-Type: application/json, application/yaml,
  // application/strategic-merge-patch+json, etc.
  string content_type = 6;

  // Accept header - specifies desired response format
  // e.g., "application/json;as=Table;v=v1;g=meta.k8s.io" for table output
  string accept = 12;

  // Timeout in milliseconds. 0 = default (30s for non-watch, none for watch)
  uint32 timeout_ms = 7;

  // Set to true to cancel an active watch by request_id
  // Agent will stop the watch and send stream_end response
  bool cancel = 8;

  // Target cluster for hierarchical routing
  // The receiving agent compares this to its own cluster name:
  // - If equal: execute request locally using operator's SA
  // - If different: forward to target cluster via its subtree
  string target_cluster = 9;

  // Source identity - preserved through the entire routing chain for Cedar checks
  // Each hop can authorize based on who originally made the request
  // Works for any auth type: SA ("system:serviceaccount:ns:name"), OIDC ("user@example.com"), etc.
  string source_user = 10;
  repeated string source_groups = 11;

  // W3C Trace Context for distributed tracing
  // Format: "00-{trace_id}-{span_id}-{flags}" (32-char trace_id, 16-char span_id)
  // See: https://www.w3.org/TR/trace-context/
  string traceparent = 13;
  // Additional vendor-specific trace information
  string tracestate = 14;
}

// Agent → Cell: Response from child's K8s API
message KubernetesResponse {
  // Correlates to KubernetesRequest.request_id
  string request_id = 1;

  // HTTP status code: 200, 201, 404, 500, etc.
  uint32 status_code = 2;

  // Response body (JSON)
  // For watches: each event is a separate response message
  bytes body = 3;

  // Content-Type from K8s API response
  string content_type = 4;

  // True if this is a streaming watch response
  // Client should expect multiple responses with same request_id
  bool streaming = 5;

  // True if this is the final message in a stream
  // After this, no more responses for this request_id
  bool stream_end = 6;

  // Error message for internal errors (not K8s API errors)
  // K8s API errors come through status_code + body
  string error = 7;
}

// =============================================================================
// Distributed Move Messages (clusterctl move semantics over gRPC)
// =============================================================================

// Cell → Agent: Batch of objects to create
//
// Objects are sent in topological order (owners before dependents).
// Each batch can be created in parallel since their owners are already created.
message MoveObjectBatch {
  // Unique move operation ID
  string move_id = 1;
  // Index of this batch (0-based)
  uint32 batch_index = 2;
  // Total number of batches
  uint32 total_batches = 3;
  // Objects to create in this batch
  repeated MoveObject objects = 4;
  // Namespace to create objects in
  string target_namespace = 5;
  // Cluster name being moved
  string cluster_name = 6;
}

// A single object to be created during move
message MoveObject {
  // UID from source cluster (used for owner reference rebuilding)
  string source_uid = 1;
  // Full object manifest (JSON, transient fields stripped)
  bytes manifest = 2;
  // Owner references from source (need UID remapping)
  repeated SourceOwnerRef owners = 3;
}

// Owner reference from source cluster
message SourceOwnerRef {
  // UID from source cluster (maps to target UID after creation)
  string source_uid = 1;
  // API version of the owner
  string api_version = 2;
  // Kind of the owner
  string kind = 3;
  // Name of the owner
  string name = 4;
  // Whether this is the controller reference
  bool controller = 5;
  // Whether deletion of dependent blocks owner deletion
  bool block_owner_deletion = 6;
}

// Agent → Cell: Ack after batch created
message MoveObjectAck {
  // Correlates to CellCommand.command_id
  string request_id = 1;
  // UID mappings from source to target
  repeated UidMapping mappings = 2;
  // Errors for specific objects (partial success)
  repeated MoveObjectError errors = 3;
}

// Mapping from source UID to target UID
message UidMapping {
  // UID from source cluster
  string source_uid = 1;
  // UID assigned by target cluster
  string target_uid = 2;
}

// Error for a specific object in a batch
message MoveObjectError {
  // Source UID of the failed object
  string source_uid = 1;
  // Error message
  string message = 2;
  // Whether this error is retryable
  bool retryable = 3;
}

// Cell → Agent: All batches sent, unpause target
message MoveComplete {
  // Move operation ID
  string move_id = 1;
  // Cluster name (for finding Cluster resources to unpause)
  string cluster_name = 2;
  // Namespace where resources were created
  string target_namespace = 3;
  // Distributable resources to apply after CAPI resources are imported
  DistributableResources resources = 4;
  // Additional manifests to apply (e.g., CiliumNetworkPolicy)
  repeated bytes manifests = 5;
}

// Agent → Cell: Unpause done, safe to delete source
message MoveCompleteAck {
  // Correlates to CellCommand.command_id
  string request_id = 1;
  // Whether unpause succeeded
  bool success = 2;
  // Error message if success=false
  string error = 3;
  // Total resources created
  int32 resources_created = 4;
}

// =============================================================================
// Subtree State Bubbling Messages
// =============================================================================

// Agent → Cell: State of this cluster's subtree
//
// Each cluster bubbles up its subtree membership to its parent. This enables:
// - Routing: parent knows which child agent connection routes to which clusters
// - Authorization: Cedar policies can reference cluster hierarchy
//
// Protocol:
// - On connect: Agent sends full subtree state (is_full_sync=true)
// - On change: Agent sends delta with added/removed clusters
// - Parent aggregates and bubbles up to its own parent
message SubtreeState {
  // Clusters in this subtree (including self)
  repeated SubtreeCluster clusters = 1;
  // Services in this subtree (future: for service mesh routing)
  repeated SubtreeService services = 2;
  // True on initial connect (full state), false for incremental updates
  bool is_full_sync = 3;
}

// Information about a cluster in the subtree
message SubtreeCluster {
  // Cluster name (unique identifier)
  string name = 1;
  // Immediate parent cluster name (empty for root)
  string parent = 2;
  // True if this cluster is being removed (delta only)
  bool removed = 3;
  // Cluster phase (Pending, Provisioning, Ready, etc.)
  string phase = 4;
  // Labels for policy matching
  map<string, string> labels = 5;
}

// Information about a service in the subtree (future use)
message SubtreeService {
  // Service name
  string name = 1;
  // Namespace
  string namespace = 2;
  // Cluster hosting this service
  string cluster = 3;
  // True if this service is being removed
  bool removed = 4;
  // Labels for policy matching
  map<string, string> labels = 5;
}

// =============================================================================
// Exec/Attach/PortForward Messages (HTTP upgrade over gRPC)
// =============================================================================

// Cell → Agent: Initiate an exec/attach/portforward session
//
// Exec, attach, and portforward require HTTP upgrade (SPDY/WebSocket) which
// is fundamentally different from watch (chunked HTTP). This message initiates
// a bidirectional streaming session over the gRPC tunnel.
message ExecRequest {
  // Unique session ID for correlation (UUID)
  string request_id = 1;
  // API path: /api/v1/namespaces/ns/pods/pod/exec
  string path = 2;
  // Query string: command=sh&stdin=true&stdout=true&tty=true
  string query = 3;
  // Target cluster for hierarchical routing
  string target_cluster = 4;
  // Source identity (preserved through routing chain for Cedar)
  string source_user = 5;
  repeated string source_groups = 6;
}

// Bidirectional: Multiplexed stream data
//
// Used in both directions:
// - Cell → Agent: stdin data from client (ExecData in CellCommand.exec_stdin)
// - Agent → Cell: stdout/stderr from container (ExecData in AgentMessage.exec_data)
//
// Stream IDs follow SPDY conventions:
// - 0: stdin (client → container)
// - 1: stdout (container → client)
// - 2: stderr (container → client)
// - 3: error (protocol errors)
// - 255: resize (terminal size changes, prefer ExecResize)
message ExecData {
  // Session ID (correlates to ExecRequest.request_id)
  string request_id = 1;
  // Stream identifier (0=stdin, 1=stdout, 2=stderr, 3=error)
  uint32 stream_id = 2;
  // Raw data bytes
  bytes data = 3;
  // True if this is the final message for this stream
  // When all streams are ended, the session is complete
  bool stream_end = 4;
}

// Cell → Agent: Terminal resize event
//
// Sent when the client terminal is resized. The agent forwards this
// to the container's PTY.
message ExecResize {
  // Session ID
  string request_id = 1;
  // New terminal width in columns
  uint32 width = 2;
  // New terminal height in rows
  uint32 height = 3;
}

// Cell → Agent: Cancel exec session
//
// Sent when the client disconnects (WebSocket close) to signal the agent
// to terminate the exec session and clean up resources.
message ExecCancel {
  // Session ID to cancel
  string request_id = 1;
}
